---
title: "Random forest"
subtitle: "Series 2.2 - building model"
author: Jennifer HY Lin
date: '2023-10-25'
draft: true
categories: 
    - Machine learning projects
    - Tree models
    - Pandas
    - Scikit-learn
    - ChEMBL database
    - Python
jupyter: python3
format: html
bibliography: references.bib
---

##### **What is a random forest?**

The [decision tree model built last time](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/3_model_build.html) was purely based on one model on its own, which often might not be as accurate as we've hoped for. If we're thinking along the line mathematically or statistically to improve the model, we would then think about using the average of multiple models [@breiman1998] to see if this output would better reflect the real-life scenario. This model averaging approach was in fact constantly used in our lives with a common example such as using majority votes in elections or decision-making processes.

The same model averaging concept was also used in random forest [@breiman2001], which as the name suggested, was composed of many decision tree models forming a forest. To focus on each tree in the forest, each tree model would be making its own model prediction. By having multiple predictions since we have multiple trees, the average obtained from these predictions would produce one single result in the end. The advantage of this was that it improved the accuracy of the prediction by reducing variances, and also minimised the problem of overfitting the model if it was purely based on one model only (section 1.11.2.1. Random Forests from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles) might help to explain this further).

The "random" part of the random forest was introduced in two ways. The first one was via using bootstrap samples, which was also known as bagging or bootstrap aggregating [@bruce2020], where samples were drawn with replacements within the training datasets for each tree built in the ensemble (the perturb-and-combine technique [@breiman1998]). While bootstrap sampling was happening, randomness was also incorporated at the same time into the training sets. The second way randomness was introduced was by using a random subset of features for splitting at the nodes, or a full set of features could also be used instead (although this was generally not recommended). The main goal was to achieve best splits at each node.

<br>

##### **Random forest in *scikit-learn***

*Scikit-learn* had two main types of random forest classes - [ensemble.RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [ensemble.RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). When to use which method would depend on the target values. The easiest thing to do was to decide whether the target variables had class labels (binary types or non-continuous variables e.g. yes or no) or continuous variables, which in this case, if I were to continue using the same dataset from last series of blog posts, it would be a continuous variable or feature, pKi, the inhibition constant. 

There were also two alternative random forest methods in *scikit-learn*, which were ensemble.RandomTreesEmbedding() and ensemble.ExtraTreesClassifier() or ensemble.ExtraTreesRegressor(). The difference for RandomTreesEmbedding() was that it was an unsupervised method that used data transformations (details can be accessed from section 1.11.2.6. on "Totally Random Trees Embedding" in [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)). I have not planned on using this unsupervised method in this post, but thought to mention it as it might be useful for others in other different ML scenarios. On the other side, there was also an option to use ExtraTreesClassifier() or ExtraTreesRegressor() to generate extremely randomised trees that would go for another level up in randomness (more deatils in section 1.11.2.2. on Extremely Randomized Trees from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)). The main difference for this type of random forest was that the while there was already a random subset of feature selection used (with an intention to select the most discerning features), more randomness were added by using purely randomly generated splitting rules for picking features at the nodes.

<br>

##### **Building a random forest regressor model using *scikit-learn***

Importing all the required libraries.

```{python}
import pandas as pd
import seaborn as sns
import sklearn
from sklearn.ensemble import RandomForestRegressor
# Showing version of *scikit-learn* used (seemed to have forgotten to include this last time)
print(sklearn.__version__)
```

Importing dataset that was preprocessed from last time.

```{python}
data = pd.read_csv("ache_2d_chembl.csv")
data.drop(columns = ["Unnamed: 0"], inplace=True)
# Preparing data for compounds with max phase as "null"
# Convert max phase with "NaN" to "null"
data["max_phase"].fillna("null", inplace=True)
data.head()
```

<br>

###### **Preparing training data**

Setting up X and y variables.

Array X (no. of samples, no. of features) - keeping it the same as how it was in the decision tree series.

Note: It's usually a good practice to copy the original data or dataframe for further data manipulations to avoid any unnecessary changes to the original dataset (this was not used in the decision tree posts but since I'm going to use the same set of data again I'd better do this here.)

```{python}
# X variables (molecular features)---
# Make a copy of the original dataframe first
data_mp4 = data.copy()
# Selecting all max phase 4 compounds
data_mp4 = data_mp4[data_mp4["max_phase"] == 4]
print(data_mp4.shape)
data_mp4.head()
```

```{python}
# Select molecular features for X variable
X_mp4_df = data_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]

print(X_mp4_df.shape)
X_mp4_df.head()
```

```{python}
# Convert X_mp4_df to numpy array
X_mp4 = X_mp4_df.to_numpy()
X_mp4
```

Array y (no. of samples or target values) - also keeping this the same as the one from the decision tree series.

```{python}
# y variable (target outcome - pKi)
y_mp4_df = data_mp4["pKi"]
y_mp4_df
```

```{python}
# Convert y_mp4_df to numpy array
y_mp4 = y_mp4_df.to_numpy()
y_mp4
```

Followed by fitting RandomForestRegressor() on these X and y variables.

```{python}
# n_estimators = 100 by default
# note: if wanting to use whole dataset - switch off "bootstrap" parameter by using "False"
rfreg = RandomForestRegressor(max_depth=3, random_state=1)
rfreg.fit(X_mp4, y_mp4)
```

<br>

###### **Preparing testing data**

```{python}
# Compounds with max phase as "null"
data_mp_null = data.copy()
# Selecting all max phase "null" compounds
data_mp_null = data_mp_null[data_mp_null["max_phase"] == "null"]
print(data_mp_null.shape)
data_mp_null.head() 
```

```{python}
# Set up X test variable with the same molecular features
X_mp_test_df = data_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]

# Convert X test variables from df to arrays
X_mp_test = X_mp_test_df.to_numpy()

X_mp_test
```

<br>

###### **Using trained model for prediction on testing data**

***Might need to move this to be after the "Feature importances" section*** ?link to model evaluation

```{python}
# Predict pKi values for the compounds with "null" max phase
# using the training model rfreg 
print(rfreg.predict(X_mp_test))
```

<br>

##### **Feature importances**

###### **feature_importances_ attribute from *scikit-learn***

```{python}
# Compute feature importances on rfreg training model
feature_imp = rfreg.feature_importances_
```

```{python}
# Check what feature_imp looks like
feature_imp
```

```{python}
# Convert the feature_imp array into dataframe
feature_imp_df = pd.DataFrame(feature_imp)
#feature_imp_df

# Obtain feature names via column names of dataframe
# Rename the index as "features"
feature = X_mp4_df.columns.rename("features")

# Convert the index to dataframe
feature_name_df = feature.to_frame(index = False)

# Concatenate feature_imp_df & feature_name_df
feature_df = pd.concat([feature_imp_df, feature_name_df], axis=1)

# Rename the column for feature importances
feature_df = feature_df.rename(columns = {0: "feature_importances"})

# Sort values of feature importances in descending order
feature_df = feature_df.sort_values("feature_importances", ascending=False)
```

```{python}
# Seaborn bar plot
sns.barplot(feature_df, x = "feature_importances", y = "features")
```

An alternative way to plot was via Matplotlib directly (note: Seaborn also uses Matplotlib as well, so the plots are pretty similar). The code below were probably a bit more straightforward but without axes named and values were not sorted.

```{python}
# Matplotlib plot
from matplotlib import pyplot as plt
plt.barh(X_mp4_df.columns, rfreg.feature_importances_)
```

<br>

##### **Model performance evaluation or hyperparameter tuning**

```{python}
# Import additional ibraries
from numpy import mean, std
# RepeatedStratifiedKFold usually for binary or multi-class labels - ref link: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold
from sklearn.model_selection import cross_val_score, RepeatedKFold
```

* Cross validations & hyperparameter tuning 
1. number of trees (n_estimators) 

```{python}
# ---Evaluate a list of models with different number of trees---

# Define dataset by using the same training dataset as above---
X, y = X_mp4, y_mp4

# Define function to generate a list of models with different no. of trees---
def models():
    # Create empty dictionary (key, value pairs) for models
    models = dict()
    # Test different number of trees to evaluate
    no_trees = [50, 100, 250, 500, 1000]
    for n in no_trees:
        models[str(n)] = RandomForestRegressor(n_estimators=n)
    return models


# Define function to evaluate a single model using cross-validation---
def evaluate(model, X, y):
    # Define evaluation process
    cross_val = RepeatedKFold(n_splits=10, n_repeats=15, random_state=1)
    # Run evaluation process & collect cv scores
    # Since estimator/model was based on DecisionTreeRegressor, using neg_mean_squared_error metric
    # n_jobs = -1 meaning using all processors to run jobs in parallel
    scores = cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=cross_val, n_jobs=-1)
    return scores


# Evaluate results---
# Run models with different RepeatedKFold & different no. of tress
# with results shown as diff. trees with calculated mean cv scores & std

# Obtain diff. models with diff. trees via models function
models = models()

# Create empty lists for results & names
results, names = list(), list()

# Create a for loop to iterate through the list of diff. models
for name, model in models.items():
    # Run the cross validation scores via evaluate function
    scores = evaluate(model, X, y)
    # Collect results
    results.append(scores)
    # Collect names (different no. of trees)
    names.append(name)
    # Show the average mean squared errors and corresponding standard deviations 
    # for each model with diff. no. of trees
    print((name, mean(scores), std(scores)))
```

Best model performance lied in 500 trees with a average mean squared error score of 1.666. The negated version of the same value was due to how the scoring parameter source code was written in scikit-learn, which was written this way to take into account of both *scoring* functions and *loss* functions (please see provided links below). When the number of trees went past 500 and reaching 1000, we could see an increase in the average mean squared error (the value being less negative), meaning the error increased.

* Links to help understanding neg_mean_squared_error:

1. scikit-learn source code - https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/metrics/_scorer.py#L624

2. StackOverflow answer - https://stackoverflow.com/questions/48244219/is-sklearn-metrics-mean-squared-error-the-larger-the-better-negated

```{python}
# Matplotlib boxplots for each no. of tree model with average mean squared errors shown
plt.boxplot(results, labels=names, showmeans=True)
plt.show()
```

```{python}
# Try Seaborn version too

```

```{python}
#sklearn.metrics.get_scorer_names() - to show all scoring metrics
# URL link: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
```

Potentially may also look at:
- tree depths (max_depth)
- number of samples (max_samples) 
- ?number of features (max_features) (can mention using RDKit's version to generate molecular features which would provide 209)

* Try keeping post as short and as succinct as possible!

* Plots - Black-box ML e.g. if compariing clogp vs. pKi? (unlike white-box ML for decision tree)

* Other options available in Scikit-learn ensemble methods e.g. voting classifier/regressor or stacking models to reduce biases 
