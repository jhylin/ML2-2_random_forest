---
title: "Random forest"
subtitle: "Series 2.2 - building model"
author: Jennifer HY Lin
date: '2023-10-13'
draft: true
categories: 
    - Machine learning projects
    - Tree models
    - Pandas
    - Scikit-learn
    - ChEMBL database
    - Python
jupyter: python3
format: html
bibliography: references.bib
---

##### **What is a random forest?**

The [decision tree model built last time](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/3_model_build.html) was purely based on one model on its own, which often might not be as accurate as we've hoped for. If we're thinking along the line mathematically or statistically to improve the model, we would then think about using the average of multiple models [@breiman1998] to see if this output would better reflect the real-life scenario. This model averaging approach was in fact constantly used in our lives with a common example such as using majority votes in elections or decision-making processes.

The same model averaging concept was also used in random forest [@breiman2001], which as the name suggested, was composed of many decision tree models forming a forest. To focus on each tree in the forest, each tree model would be making its own model prediction. By having multiple predicitions since we have multiple trees, the average obtained from these predictions would produce one single result in the end. The advantage of this was that it improved the accuracy of the prediction by reducing variances, and also minimised the problem of overfitting the model if it was purely based on one model only (section 1.11.2.1. Random Forests from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles) might help to explain this further).

The "random" part of the random forest was introduced in two ways. The first one was via using bootstrap samples, which was also known as bagging or bootstrap aggregating [@bruce2020], where samples were drawn with replacements within the training datasets for each tree built in the ensemble (the perturb-and-combine technique [@breiman1998]). While bootstrap sampling was happening, randomness was also incorporated at the same time into the training sets. The second way randomness was introduced was by using a random subset of features for splitting at the nodes, or a full set of features could also be used instead. The main goal was to achieve best splits at each node.

<br>

##### **Random forest in *scikit-learn***

*Scikit-learn* had two main types of random forest methods - [ensemble.RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [ensemble.RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). When to use which method would depend on the target values. The easiest thing to do was to decide whether the target variables had class labels (binary types or non-continuous variables e.g. yes or no) or continuous variables, which in this case, if I were to continue using the same dataset from last series of blog posts, it would be a continuous variable or feature, pKi, the inhibition constant. 

There were also two alternative random forest methods in *scikit-learn*, which were ensemble.RandomTreesEmbedding() and ensemble.ExtraTreesClassifier() or ensemble.ExtraTreesRegressor(). The difference for RandomTreesEmbedding() was that it was an unsupervised method that used data transformations (details can be accessed from section 1.11.2.6. on "Totally Random Trees Embedding" in [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)). I have not planned on using this unsupervised method in this post, but thought to mention it as it might be useful for others in other different ML scenarios. On the other side, there was also an option to use ExtraTreesClassifier() or ExtraTreesRegressor() to generate extremely randomised trees that would go for another level up in randomness (more deatils in section 1.11.2.2. on Extremely Randomized Trees from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)). The main difference for this type of random forest was that the while there was already a random subset of feature selection used (with an intention to select the most discerning features), more randomness were added by using purely randomly generated splitting rules for picking features at the nodes.

<br>

##### **Building a random forest regressor model***

Importing all the required libraries.

```{python}
import pandas as pd
import sklearn
from sklearn.ensemble import RandomForestRegressor
```

Showing the version of *scikit-learn* used (I seemed to have forgotten to include this last time).

```{python}
print(sklearn.__version__)
```

Importing dataset that was preprocessed from last time.

```{python}
data = pd.read_csv("ache_2d_chembl.csv")
data.drop(columns = ["Unnamed: 0"], inplace=True)
data.head()
```

Setting up X and y variables.

```{python}
# X variables (molecular features)


# y variable (target outcome - pKi)

```

- using RandomForestRegressor()

- array X (no. of samples, no. of features) vs. array y (no. of samples or target values) 

<br>

*Draft plans*: 

- Hyperparameter tuning needed 

- Plots - Black-box ML (unlike white-box ML for decision tree)

- Other options available in Scikit-learn ensemble methods e.g. voting classifier/regressor or stacking models to reduce biases 
