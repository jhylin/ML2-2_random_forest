---
title: "Random forest"
subtitle: "Series 2.2 - model building, feature importances & hyperparameter tuning"
author: Jennifer HY Lin
date: '2023-11-17'
draft: true
categories: 
    - Machine learning projects
    - Tree models
    - Pandas
    - Scikit-learn
    - ChEMBL database
    - Python
jupyter: python3
format: html
bibliography: references.bib
---

##### **Quick overview of this post**

-   Introduction to random forest
-   Random forest methods or classes in *scikit-learn*
-   Random forest regressor model in *scikit-learn*
-   Training and testing data splits
    -   ChEMBL-assigned max phase splits
    -   Imbalanced learning regression and max phase splits
-   Scoring metrics of trained models
-   Feature importances in dataset
    -   feature_importances_attribute in *scikit-learn*
    -   permutation_importance function in *scikit-learn*
    -   SHAP approach
-   Hyperparameter tuning

<br>

##### **What is a random forest?**

The [decision tree model built last time](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/3_model_build.html) was purely based on one model on its own, which often might not be as accurate or reflective in real-life. To improve the model, we would then consider using the average of multiple models [@breiman1998] to see if this average output would provide a more realistic outcome. This model averaging approach was constantly used in our lives, for example, using majority votes in elections or decision-making steps.

The same model averaging concept was also used in random forest [@breiman2001], which as the name suggested, was composed of many decision trees (models) forming a forest. Each tree model would be making its own model prediction. By accruing multiple predictions since we have multiple trees, the average obtained from these predictions would produce one single result in the end. The advantage of this was that it improved the accuracy of the prediction by reducing variances, and also minimised the problem of overfitting the model if it was purely based on one model only (more details in section 1.11.2.1. Random Forests from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)).

The "random" part of the random forest was introduced in two ways. The first one was via using bootstrap samples, which was also known as bagging or bootstrap aggregating [@bruce2020], where samples were drawn with replacements within the training datasets for each tree built in the ensemble (also known as the perturb-and-combine technique [@breiman1998]). While bootstrap sampling was happening, randomness was also incorporated into the training sets at the same time. The second way randomness was introduced was by using a random subset of features for splitting at the nodes, or a full set of features could also be used (although this was generally not recommended). The main goal here was to achieve best splits at each node.

<br>

##### **Random forest in *scikit-learn***

*Scikit-learn* had two main types of random forest classes - [ensemble.RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [ensemble.RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). When to use which class would depend on the target values. The easiest thing to do was to decide whether the target variables had class labels (binary types or non-continuous variables e.g. yes or no, or other different categories to be assigned) or continuous (numerical) variables, which in this case, if I were to continue using the same dataset from the decision tree series, it would be a continuous variable or feature, pKi, the inhibition constant.

There were also two other, alternative random forest methods in *scikit-learn*, which were ensemble.RandomTreesEmbedding() and ensemble.ExtraTreesClassifier() or ensemble.ExtraTreesRegressor(). The difference for RandomTreesEmbedding() was that it was an unsupervised method that used data transformations (more details from section 1.11.2.6. on "Totally Random Trees Embedding" in [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding)). On the other side, there was also an option to use ExtraTreesClassifier() or ExtraTreesRegressor() to generate extremely randomised trees that would go for another level up in randomness (more deatils in section 1.11.2.2. on Extremely Randomized Trees from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)). The main difference for this type of random forest was that while there was already a random subset of feature selection used (with an intention to select the most discerning features), more randomness were added on top of this by using purely randomly generated splitting rules for picking features at the nodes.

<br>

##### **Building a random forest regressor model using *scikit-learn***

As usual, all the required libraries were imported first.

```{python}
# Add more libraries from below!
import pandas as pd
import seaborn as sns
import sklearn
from sklearn.ensemble import RandomForestRegressor
# Showing version of *scikit-learn* used
print(sklearn.__version__)
```

Importing dataset that was preprocessed from last time. Link to data source: [first decision tree post](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/1_data_col_prep.html).

```{python}
data = pd.read_csv("ache_2d_chembl.csv")
data.drop(columns = ["Unnamed: 0"], inplace=True)
# Preparing data for compounds with max phase as "null"
# Convert max phase with "NaN" to "null"
data["max_phase"].fillna("null", inplace=True)
data.head()
```

<br>

##### **Training/testing splits**

Two approaches were used, where one was based purely on max phase split (between max phase null and 4), which was used last time in the decision tree series, and the other one was using the same max phase split but with an ImbalancedLearningRegression method added on top of it.

<br>

###### **Preparing training data using max phase split**

X variable was set up first via the dataframe, and then converted to a NumPy array, that consisted of the number of samples and number of features. This was kept the same as how it was in the decision tree posts.

Note: It's usually recommended to copy the original data or dataframe for further data manipulations to avoid any unnecessary changes to the original dataset (this was not used in the decision tree posts, but since I'm going to use the same set of data again I'm doing it here now.)

```{python}
# X variables (molecular features)
# Make a copy of the original dataframe first
data_mp4 = data.copy()
# Selecting all max phase 4 compounds
data_mp4 = data_mp4[data_mp4["max_phase"] == 4]
print(data_mp4.shape)
data_mp4.head()
```

```{python}
# Select molecular features for X variable
X_mp4_df = data_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]

print(X_mp4_df.shape)
X_mp4_df.head()
```

```{python}
# Convert X_mp4_df to numpy array
X_mp4 = X_mp4_df.to_numpy()
X_mp4
```

Again, y variable was arranged via the dataframe as well, and converted into a NumPy array, that consisted of number of samples only as this was the target variable, which was also kept the same as the decision tree series.

```{python}
# y variable (target outcome - pKi)
y_mp4_df = data_mp4["pKi"]
y_mp4_df
```

```{python}
# Convert y_mp4_df to numpy array
y_mp4 = y_mp4_df.to_numpy()
y_mp4
```

<br>

###### **Training model with the training dataset using max phase split only**

Both X and y variables were used to fit the RandomForestRegressor() estimator.

```{python}
# n_estimators = 100 by default
# note: if wanting to use whole dataset - switch off "bootstrap" parameter by using "False"
rfreg = RandomForestRegressor(max_depth=3, random_state=1)
rfreg.fit(X_mp4, y_mp4)
```

<br>

###### **Preparing testing data using max phase split only**

Testing data was mainly based on compounds with max phase assigned as "0" or "null" after I renamed it above.

```{python}
# Compounds with max phase as "null"
data_mp_null = data.copy()
# Selecting all max phase "null" compounds
data_mp_null = data_mp_null[data_mp_null["max_phase"] == "null"]
print(data_mp_null.shape)
data_mp_null.head() 
```

```{python}
# Set up X test variable with the same molecular features
X_mp_test_df = data_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]

# Convert X test variables from df to arrays
X_mp_test = X_mp_test_df.to_numpy()

X_mp_test
```

<br>

###### **Training/testing splits using ImbalancedLearningRegression and max phase splits**

I didn't really pay a lot of attentions when I was doing data splits in the decision tree series, as my main focus was on building a single tree in order to fully understand and see what could be derived from just one tree. Now, when I reached this series on random forest, I realised I forgot to mention in the last series that data splitting was actually very crucial on model performance and also could influence outcome predictions. It could also become quite complicated as more approaches were available to split the data. Also, the way the data was splitted could produce different outcomes.

After I've splitted the same dataset based on compounds' max phase assignments in ChEMBL and also fitted the training data on the random forest regressor, I went back and noticed that the training and testing data were very imbalanced and I probably should do something about it before fitting them onto another model.

At this stage, I went further to look into whether imbalanced datasets should be addressed in regression tasks, and did a surface search online. So based on common ML concensus, imbalanced dataset was more applicable to classification tasks (e.g. binary labels or multi-class labels), rather than regression problems. However, recent ML research looked into the issue of imbalanced datasets in regression. This [blog post](https://neptune.ai/blog/how-to-deal-with-imbalanced-classification-and-regression-data) mentioned a few studies that looked into this type of problem, and I thought they were very interesting and worth a mention at least. One of them that I've looked into was SMOTER, which was based on synthetic minority over-sampling technique (SMOTE)[@chawla2002], and was named this way because it was basically a SMOTE for regression (hence SMOTER)[@torgo2013]. Synthetic minority over-sampling technique for regression with Gaussian noise (SMOGN)[@smogn] was another technique that was built upon SMOTER, but with Gaussian noises added. This has subsequently led me to ImbalancedLearningRegression library [@wu2022imbalancedlearningregression], which was a variation of SMOGN. This was the one used on my imbalanced dataset, shown in the section below.

A simple flow diagram was drawn below showing the evolution of different techniques when dealing with imbalanced datasets in classification (SMOTE) and regression (SMOTER, SMOGN and ImbalancedLearningRegression):

```{mermaid}
flowchart LR
  A(SMOTE) --> B(SMOTER)
  B --> C(SMOGN)
  C --> D(ImbalancedLearningRegression)
```

GitHub repository for ImbalancedLearningRegression package is available [here](https://imbalancedlearningregression.readthedocs.io/en/latest/intro.html).

Also, I just wanted to mention that these were not the only techniques available for treating imbalanced datasets in regression, as I'm aware there were other ones in the literature, but I only had time to cover these a bit here for now.

Another really useful open-source resource for treating imbalanced datasets in classification problems was also available - [imbalance-learn library](https://imbalanced-learn.org/stable/index.html#).

```{python}
# Original dataset - checking shape again
print(data.shape)
data.head()
```

So my little test on using ImbalancedLearningRegression package started from down below.

```{python}
# Trial using ImbalancedLearningRegression
import ImbalancedLearningRegression as iblr

data = data.copy()

# Using random over-sampling on the original dataset (pre-max phase split)
# data_ro = iblr.ro(data = data, y = "pKi")
# print(data_ro.shape)
```

```{python}
# Gaussian noise
data_gn = iblr.gn(data = data, y = "pKi", pert = 1)
print(data_gn.shape)
```

```{python}
# Gaussian Noise
# Followed by max phase split, where 4 = training
data_gn_mp4 = data_gn[data_gn["max_phase"] == 4]
data_gn_mp4
print(data_gn_mp4.shape)
```

```{python}
# Random over-sampling
# Followed by max phase split, where 4 = training
# data_ro_mp4 = data_ro[data_ro["max_phase"] == 4]
# data_ro_mp4
# print(data_ro_mp4.shape)
```

```{python}
# Gaussian Noise
# Also splitted max phase null compounds = testing
data_gn_mp_null = data_gn[data_gn["max_phase"] == "null"]
data_gn_mp_null
# Show shape of df for max phase null compounds
print(data_gn_mp_null.shape)
```

```{python}
# Random over-sampling
# # Also splitted max phase null compounds = testing
# data_ro_mp_null = data_ro[data_ro["max_phase"] == "null"]
# data_ro_mp_null
# # Show shape of df for max phase null compounds
# print(data_ro_mp_null.shape)
```

Random over-sampling actually oversampled the max phase null compounds (sample size increased), and keeping all 10 max phase 4 compounds.

Under-sampling removed all of the max phase 4 compounds (which was most likely not the best option), with max phase null compounds also reduced in size too.

I also used the Gauissian Noise sampling and it reduced max phase 4 compounds slightly, and increased the max phase null compounds.

The change in the distribution of pKi values between the original and sample-modified datasets could be seen in the plot below.

```{python}
# Quick look at how the pKi values differed 
# after applying iblr to dataset

import matplotlib.pyplot as plt

# Plot target variable distributions
sns.kdeplot(data["pKi"], label = "Original")
sns.kdeplot(data_gn["pKi"], label = "Modified")
plt.legend(labels = ["Original", "Modified"])
```

Range of pKi values for max phase 4 compounds was between 4 and 8.

```{python}
# Checking pKi value range for max phase 4 compounds
#data_mp4
```

Preparing training data.

```{python}
# Select molecular features for X variable
X_mp4_gn_df = data_gn_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]

print(X_mp4_gn_df.shape)
X_mp4_gn_df.head()
```

```{python}
X_mp4_gn = X_mp4_gn_df.to_numpy()
```

```{python}
# y variable (target outcome - pKi)
y_mp4_gn_df = data_gn_mp4["pKi"]

y_mp4_gn = y_mp4_gn_df.to_numpy()
y_mp4_gn
```

Fitting iblr-Gaussian noise training data onto another random forest regressor model.

```{python}
# n_estimators = 100 by default
# note: if wanting to use whole dataset - switch off "bootstrap" parameter by using "False"
rfreg_gn = RandomForestRegressor(max_depth=3, random_state=1)
rfreg_gn.fit(X_mp4_gn, y_mp4_gn)
```

Preparing testing data.

```{python}
# Set up X test variable with the same molecular features
X_mp_gn_test_df = data_gn_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]

# Convert X test variables from df to arrays
X_mp_gn_test = X_mp_gn_test_df.to_numpy()

X_mp_gn_test
```

<br>

###### **Using trained model for prediction on testing data**

Predicting max phase-splitted data only.

```{python}
# Predict pKi values for the compounds with "null" max phase
# using the training model rfreg 
# Uncomment code below to print prediction result
#print(rfreg.predict(X_mp_test))

# or use:
y_mp_test = rfreg.predict(X_mp_test)
```

Predicting iblr-gn data with max phase splits.

```{python}
y_mp_gn_test = rfreg_gn.predict(X_mp_gn_test)
```

<br>

###### **Accuracy/scoring metrics of trained models**

Checking model accuracy for both training and testing datasets was actually recommended to occur before moving onto finding out the feature importances. A *scikit-learn* explanation for this could be found in the section on ["Permutation feature importance"](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance).

```{python}
# Training set accuracy
print(f"Random forest regressor training accuracy: {rfreg.score(X_mp4, y_mp4):.2f}")

# Testing set accuracy
print(f"Random forest regressor testing accuracy: {rfreg.score(X_mp_test, y_mp_test):.2f}")
```

So it looked like both the training and testing accuracies for the random forest regressor model (rfreg) were quite high, meaning that the model was able to remember the molecular features well from the training set (the tiny sample of 10 compounds), and the model was able to apply them to the testing set (which should contain about 400s of compounds) as well, in order to make predictions on the target value of pKi. So this has confirmed that the model was indeed making predictions (rather than not making any at all, which meant there might be no point in finding out which features were important in the data, so it was a good checking point during the random forest model building exercise), therefore, we could proceed to the next step of generating some feature importances, which were useful information to fill in the bigger story i.e. which features were pivotal for influencing the pKi values of prescription drugs targeting AChE?

```{python}
# iblr-Gaussian noise & max phase splitted data
# Training set accuracy
print(f"Random forest regressor training accuracy: {rfreg_gn.score(X_mp4_gn, y_mp4_gn):.2f}")

# Testing set accuracy
print(f"Random forest regressor testing accuracy: {rfreg_gn.score(X_mp_gn_test, y_mp_gn_test):.2f}")
```

```{python}
data_mp_null.head()
```

```{python}
#data_mp_null = data_mp_null[data_mp_null["max_phase"] == "null"]
y_true = data_mp_null["pKi"]
y_true = y_true.to_numpy(copy=True)
```

```{python}
type(y_true)
#y_true
```

```{python}
type(y_mp_test)
```

Mean squared error between y_true (max phase null compounds' pKi values) and y_pred

MSE - closer to zero, the better the model is (less errors present)

```{python}
from sklearn.metrics import mean_squared_error
mean_squared_error(y_true, y_mp_test)
```

R2 - closer to 1, the better the model is (negative likely model was not performing as well as expected or as quoted from scikit-learn "arbitrarily worse")

```{python}
from sklearn.metrics import r2_score
r2_score(y_true, y_mp_test)
```

Because the data was re-sampled in a iblr-gn way, the size of array would be different from the original dataset, so grabbing the iblr-gn modified data.

```{python}
y_true_gn = data_gn_mp_null["pKi"]
y_true_gn = y_true_gn.to_numpy(copy=True)
```

```{python}
# R squared for iblr-gn data
#type(y_true_gn)
r2_score(y_true_gn, y_mp_gn_test)
```

```{python}
# MSE for iblr-gn data
mean_squared_error(y_true_gn, y_mp_gn_test)
```

Well, it appeared iblr-gn data might not offer much advantage to the original max phase split method, even the max phase split method wasn't that great either.

<br>

##### **Feature importances**

There were two types of feature importances available in *scikit-learn*, which I've described below.

<br>

###### **feature_importances\_ attribute from *scikit-learn***

The impurity-based feature importances (also known as Gini importance).

```{python}
# Compute feature importances on rfreg training model
feature_imp = rfreg.feature_importances_
```

```{python}
# Check what feature_imp looks like (an array)
feature_imp
```

```{python}
# Initial code for converting array into df---

# Convert the feature_imp array into dataframe
feature_imp_df = pd.DataFrame(feature_imp)
#feature_imp_df

# Obtain feature names via column names of dataframe
# Rename the index as "features"
feature = X_mp4_df.columns.rename("features")

# Convert the index to dataframe
feature_name_df = feature.to_frame(index = False)

# Concatenate feature_imp_df & feature_name_df
feature_df = pd.concat([feature_imp_df, feature_name_df], axis=1)

# Rename the column for feature importances
feature_df = feature_df.rename(columns = {0: "feature_importances"})

# Sort values of feature importances in descending order
feature_df = feature_df.sort_values("feature_importances", ascending=False)
```

```{python}
# Function to convert array to df leading to plots - for use in feature_importances_ & permutation_importance

def feat_imp_plot(feat_imp_array, X_df):

    """
    Function to convert feature importance array into a dataframe, which is then used to plot a bar graph to show the feature importance ranking in the random forest model for the dataset used.

    feat_imp_array is the array obtained from the feature_importances_ attribute, after having a estimator/model fitted.

    X_df is the dataframe for the X variable, where the feature column names will be used in the plot.
    """

    # Convert the feat_imp array into dataframe
    feat_imp_df = pd.DataFrame(feat_imp_array)
    
    # Obtain feature names via column names of dataframe
    # Rename the index as "features"
    feature = X_df.columns.rename("features")

    # Convert the index to dataframe
    feature_name_df = feature.to_frame(index = False)

    # Concatenate feature_imp_df & feature_name_df
    feature_df = pd.concat(
        [feat_imp_df, feature_name_df], 
        axis=1
        ).rename(
            # Rename the column for feature importances
            columns = {0: "feature_importances"}
            ).sort_values(
                # Sort values of feature importances in descending order
                "feature_importances", ascending=False
                )
    
    # Seaborn bar plot
    sns.barplot(
        feature_df, 
        x = "feature_importances", 
        y = "features")
```

```{python}
# Testing feat_imp_plot function
feat_imp_plot(feature_imp, X_mp4_df)
```

```{python}
# Seaborn bar plot
# sns.barplot(feature_df, x = "feature_importances", y = "features")
```

An alternative way to plot was via Matplotlib directly (note: Seaborn also uses Matplotlib as well, so the plots are pretty similar). The code below were probably a bit more straightforward but without axes named and values were not sorted.

```{python}
# Matplotlib plot
from matplotlib import pyplot as plt
plt.barh(X_mp4_df.columns, rfreg.feature_importances_)
```

<br>

###### **permutation_importance function from *scikit-learn***

There were known issues with the built-in feature_importances\_ attribute in *scikit-learn*. As quoted from *scikit-learn* on [feature importance evaluation](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation):

> ... The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore do not necessarily inform us on which features are most important to make good predictions on held-out dataset. Secondly, they favor high cardinality features, that is features with many unique values. Permutation feature importance is an alternative to impurity-based feature importance that does not suffer from these flaws. ...

So here I've tried to use the model-agnostic permutation_importance function.

```{python}
from sklearn.inspection import permutation_importance

perm_result = permutation_importance(rfreg, X_mp_test, y_mp_test, n_repeats=10, random_state=1, n_jobs=2)

type(perm_result)
```

```{python}
perm_imp = perm_result.importances_mean

# Confirm it produces an array
type(perm_imp)
```

```{python}
# Trial using the function feat_imp_plot(feat_imp_array, X_df) on perm_imp result

feat_imp_plot(perm_imp, X_mp4_df)
```

It generated a different feature importance ranking, although somewhat similar as well, as shown from the horizontal bar graph.

<br>

###### **SHAP approach**

Shapley additive explanations

GitHub repo: https://github.com/shap/shap

TreeExplainer ref [@lundberg2020local2global]

Other reference: https://mljar.com/blog/feature-importance-in-random-forest/

```{python}
import shap

shap_explainer = shap.TreeExplainer(rfreg)
# X_test needs to be a dataframe (not numpy array)
# otherwise feature names won't show in plot
shap_values = shap_explainer.shap_values(X_mp_test_df)

# Horizontal bar plot
shap.summary_plot(shap_values, X_mp_test_df, plot_type = "bar")
```

```{python}
# Dot plot
shap.summary_plot(shap_values, X_mp_test_df)
```

```{python}
# Violin plot
shap.summary_plot(shap_values, X_mp_test_df, plot_type = "violin")

# Alternative option: "layered_violin"
```

<br>

##### **Hyperparameter tuning**

```{python}
# Import additional ibraries
from numpy import mean, std
# RepeatedStratifiedKFold usually for binary or multi-class labels - ref link: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold
from sklearn.model_selection import cross_val_score, RepeatedKFold
```

-   Cross validations & hyperparameter tuning
-   number of trees (n_estimators)

```{python}
# ---Evaluate a list of models with different number of trees---

# Define dataset by using the same training dataset as above---
X, y = X_mp4, y_mp4

# Define function to generate a list of models with different no. of trees---
def models():
    # Create empty dictionary (key, value pairs) for models
    models = dict()
    # Test different number of trees to evaluate
    no_trees = [50, 100, 250, 500, 1000]
    for n in no_trees:
        models[str(n)] = RandomForestRegressor(n_estimators=n)
    return models


# Define function to evaluate a single model using cross-validation---
def evaluate(model, X, y):
    # Define evaluation process
    cross_val = RepeatedKFold(n_splits=10, n_repeats=15, random_state=1)
    # Run evaluation process & collect cv scores
    # Since estimator/model was based on DecisionTreeRegressor, using neg_mean_squared_error metric
    # n_jobs = -1 meaning using all processors to run jobs in parallel
    scores = cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=cross_val, n_jobs=-1)
    return scores


# Evaluate results---
# Run models with different RepeatedKFold & different no. of tress
# with results shown as diff. trees with calculated mean cv scores & std

# Obtain diff. models with diff. trees via models function
models = models()

# Create empty lists for results & names
results, names = list(), list()

# Create a for loop to iterate through the list of diff. models
for name, model in models.items():
    # Run the cross validation scores via evaluate function
    scores = evaluate(model, X, y)
    # Collect results
    results.append(scores)
    # Collect names (different no. of trees)
    names.append(name)
    # Show the average mean squared errors and corresponding standard deviations 
    # for each model with diff. no. of trees
    print((name, mean(scores), std(scores)))
```

Best model performance would be the one with the most negative value for average mean squared error (note: the random forest algorithm was stochastic in nature, so every time it was run, it would provide different results due to random bootstrap sampling, so there wouldn't be a fixed answer). The negated version of the same value was due to how the scoring parameter source code was written in scikit-learn, which was written this way to take into account of both *scoring* functions and *loss* functions (please see provided links below). When the number of trees went past 500 and reaching 1000, we could see an increase in the average mean squared error (the value being less negative), meaning the error increased.

-   Links to help understanding neg_mean_squared_error:

1.  scikit-learn source code - https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/metrics/\_scorer.py#L624

2.  StackOverflow answer - https://stackoverflow.com/questions/48244219/is-sklearn-metrics-mean-squared-error-the-larger-the-better-negated

```{python}
# Matplotlib boxplots for each no. of tree model with average mean squared errors shown
import matplotlib.pyplot as plt
plt.boxplot(results, labels=names, showmeans=True)
plt.show()
```

Small data prep for Seaborn plot - a long-winded way of sorting data and plotting in Seaborn! Matplotlib seems much more straightforward.

```{python}
# Combine results & names lists into dataframe
cv_results = pd.DataFrame(results, index = [names])
```

```{python}
# Reset index and rename the number of trees column
cv_results = cv_results.reset_index().rename(columns={"level_0": "Number_of_trees"})
```

```{python}
# Melt the dataframe by number of trees column
cv_results = cv_results.melt(id_vars="Number_of_trees")
```

Using natural sort to sort numerical values. GitHub repo: https://github.com/SethMMorton/natsort

Otherwise, using sort_values() by itself will be sorting the numbers lexicographical order (i.e. by first digit only), which is not really in ascending order.

```{python}
# Sort by the number of trees column
from natsort import index_natsorted
import numpy as np
cv_results = cv_results.sort_values(
    by="Number_of_trees",
    key=lambda x: np.argsort(index_natsorted(cv_results["Number_of_trees"]))
)
```

```{python}
# Try Seaborn version too
sns.boxplot(cv_results, x="Number_of_trees", y="value", showmeans=True)
```

The Seaborn boxplot shown should be very similar to the Matplotlib one with also the mean scores shown for different number of trees used.

```{python}
# Show all scoring metrics - URL link: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
# sklearn.metrics.get_scorer_names() 
```

\*Not coding for the ones below (due to length of post), but they should be looked into if doing full-scale, comprehensive ML models using random forest:

-   tree depths (max_depth)

-   number of samples (max_samples) (probably won't do this as the training sample size was already very small to start with!)

-   number of features (max_features) (can mention using RDKit's version to generate molecular features which would provide 209)

-   nodes

-   Plots - Black-box ML e.g. if comparing clogp vs. pKi? (unlike white-box ML for decision tree) - or can mention that the feature importances section was necessary to shed some lights and remove some layers of the black-box style of random forest by showing which features were making impacts on the predictive models.

-   Other options available in Scikit-learn ensemble methods e.g. voting classifier/regressor or stacking models to reduce biases

-   Aim to keep post short and succinct!
